# Local Development Environment Variables
# Copy this file to .env.local and fill in the values

# ============================================
# OLLAMA (Recommended for 100% Local Setup)
# ============================================
# Set to "true" to use local Ollama instead of cloud AI
USE_LOCAL_OLLAMA=true

# Ollama server URL (default works for Docker-based Supabase)
# - If running Supabase in Docker: http://host.docker.internal:11434
# - If running Supabase natively: http://localhost:11434
OLLAMA_URL=http://host.docker.internal:11434

# Ollama model to use (must be pulled first with: ollama pull <model>)
# Recommended models:
# - llama3.2 (4GB, good balance)
# - llama3.1 (8GB, better quality)
# - mistral (4GB, fast)
OLLAMA_MODEL=llama3.2

# ============================================
# CLOUD AI (Alternative - requires API keys)
# ============================================
# Uncomment and set these if you want to use cloud AI instead of Ollama

# Gemini API Key (from Google AI Studio)
# GEMINI_API_KEY=your_gemini_api_key_here

# Or use Google AI API Key (same as Gemini)
# GOOGLE_AI_API_KEY=your_google_ai_api_key_here
