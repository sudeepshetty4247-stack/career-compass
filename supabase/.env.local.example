# =============================================
# LOCAL DEVELOPMENT - Copy to .env.local
# =============================================
# IMPORTANT: On Windows, ensure this is saved as ".env.local" not ".env.local.txt"

# ============================================
# OLLAMA - 100% Local AI (Recommended)
# ============================================
USE_LOCAL_OLLAMA=true

# Ollama server URL
# Docker Supabase: http://host.docker.internal:11434
# Native Supabase: http://localhost:11434
OLLAMA_URL=http://host.docker.internal:11434

# Model to use (MUST be pulled first!)
# FAST (recommended for most laptops):
#   tinyllama  - Fastest, works on any CPU
#   phi3       - Fast, slightly better quality
# QUALITY (needs good GPU or patience):
#   llama3.2   - Good balance
#   mistral    - Good quality
OLLAMA_MODEL=tinyllama

# ============================================
# OLLAMA PERFORMANCE TUNING
# ============================================
# Increase these if you have a powerful machine
# Decrease if you keep getting timeouts

# Timeout in milliseconds (default: 120000 = 2 min)
OLLAMA_TIMEOUT_MS=120000

# Max tokens to generate (default: 500, increase for more detailed output)
OLLAMA_NUM_PREDICT=500

# Max resume characters to send (default: 2000)
OLLAMA_MAX_RESUME_CHARS=2000

# ============================================
# CLOUD FALLBACK (Optional)
# ============================================
# Set to true to automatically use cloud AI if Ollama fails
ALLOW_CLOUD_FALLBACK=false

# ============================================
# CLOUD AI (Alternative to Ollama)
# ============================================
# To use cloud AI instead, set USE_LOCAL_OLLAMA=false
# and uncomment one of these:

# GEMINI_API_KEY=your_gemini_api_key
# GOOGLE_AI_API_KEY=your_google_ai_api_key
